{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2340435",
   "metadata": {},
   "source": [
    "# Task 2: Data Exploration and Processing\n",
    "\n",
    "## 1. Manual data inspection\n",
    "- Investigate which standard and potential new NER types are most prominent in your data set (i.e., manual data inspection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45545326",
   "metadata": {},
   "source": [
    "Following visual inspection,some of the prominent NERs found are: DATE, BODY_PART, DOSAGE, MEASUREMENT, DRUG and SYMPTOM. \n",
    "\n",
    "Examples:\n",
    " 1. DATE: 12/20/2005, 1/19/96\n",
    " 2. BODY_PART: nose, abdomen, knee\n",
    " 3. DOSAGE:  10/40 mg one a day, 0.25 micrograms a day, 50 mg twice a day, 10 ml\n",
    " 4. MEASUREMENT: 3.98 kg, 8mm, pulse of 84, blood pressure 108/65\n",
    " 5. DRUG:  Vytorin, Rocaltrol, Carvedilol, Cozaar,  Lasix\n",
    " 6. SYMPTON: erythematous, chest pain, constipated\n",
    "\n",
    "Where, DATE is a **standard NER in spacy** and the remaining ones fall in the medicine domain category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf5702",
   "metadata": {},
   "source": [
    "## 2. Apply the standard NER classifier of spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4cb76f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4127fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carla\\.conda\\envs\\llm-dev\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "c:\\Users\\carla\\.conda\\envs\\llm-dev\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f457d37",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "392028f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4966\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 1: Load Dataset\n",
    "# ============================\n",
    "dataset = load_dataset(\"argilla/medical-domain\", split=\"train\")\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad783949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our NER schema =  ['DISEASE', 'BODY_PART', 'PROCEDURE', 'FINDING', 'SYMPTOM']\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2: Define Entity Schema\n",
    "# ================================================\n",
    "# Our final gold-label schema\n",
    "CUSTOM_LABELS = [\n",
    "\n",
    "    \"DISEASE\", \"BODY_PART\", \"PROCEDURE\", \"FINDING\", \"SYMPTOM\"\n",
    "]\n",
    "\n",
    "print(\"Our NER schema = \", CUSTOM_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f45d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample_for_annotation.csv with 2 sentences\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3: Pick N samples manually\n",
    "# =========================================\n",
    "PATH_TO_ANNOTATIONS = \"ner/samples/\"\n",
    "SEED = 42 \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N_SAMPLES = 2   # select a proper number to contain 100 ground-truth NERs \n",
    "sampled_texts = []\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    row = dataset[np.random.randint(0, len(dataset))]\n",
    "    text = row[\"text\"] if \"text\" in row else row[\"content\"]\n",
    "    sent = row[\"text\"] if i==0 else None\n",
    "    sampled_texts.append(text)\n",
    "\n",
    "df_samples = pd.DataFrame({\"text\": sampled_texts})\n",
    "df_samples.to_csv(PATH_TO_ANNOTATIONS + \"sample_for_annotation.csv\", index=False)\n",
    "\n",
    "print(\"Saved sample_for_annotation.csv with\", len(df_samples), \"sentences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c1289",
   "metadata": {},
   "source": [
    "## 3. Evaluation of Standard NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc35f24",
   "metadata": {},
   "source": [
    "\n",
    "### Generate Templates for Manual Annotation. \n",
    "\n",
    " The annotation format should be: \n",
    "\n",
    "    {\n",
    "        \"sample_id\": 0, \n",
    "        \"sentence_id\": 0, \n",
    "        \"text\": \"REASON FOR THE CONSULT:,  Nonhealing right ankle stasis ulcer.,HISTORY\", \n",
    "        \"annotation\": \"[[\"Nonhealing\",\"FINDING\"],[\"right ankle\",\"BODY_PART\"],[\"stasis ulcer\",\"DISEASE\"]]\"\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c697b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 sentences: 83\n",
      "Sample 1 sentences: 89\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Generate train/test annotation template JSONL with ratio control\n",
    "# ============================================\n",
    "INPUT_CSV = PATH_TO_ANNOTATIONS + \"sample_for_annotation.csv\"\n",
    "\n",
    "# ================================\n",
    "# Parameters: control split ratio\n",
    "# ================================\n",
    "TRAIN_RATIO = 0.60     # first 60% for training（sample 0）\n",
    "TEST_RATIO  = 0.30     # first 30% for evaluation/test（sample 1）\n",
    "\n",
    "# ================================\n",
    "\n",
    "# 1. Load samples: only column \"text\"\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "df[\"sample_id\"] = df.index  # auto-index\n",
    "\n",
    "# 2. Load spaCy for sentence splitting\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Buckets\n",
    "train_items = []\n",
    "test_items = []\n",
    "\n",
    "# ===========================\n",
    "# Split sentences per sample\n",
    "# ===========================\n",
    "for _, row in df.iterrows():\n",
    "\n",
    "    sample_id = int(row[\"sample_id\"])\n",
    "    full_text = str(row[\"text\"])\n",
    "    doc = nlp(full_text)\n",
    "\n",
    "    # Extract sentences\n",
    "    sents = [s.text.strip() for s in doc.sents if len(s.text.strip()) > 0]\n",
    "    total = len(sents)\n",
    "\n",
    "    if sample_id == 0:\n",
    "        # sample 0 → train sample\n",
    "        cutoff = int(total * TRAIN_RATIO)\n",
    "        selected = sents[:cutoff]\n",
    "\n",
    "        for i, text in enumerate(selected):\n",
    "            train_items.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"sentence_id\": i,\n",
    "                \"text\": text,\n",
    "                \"annotation\": []\n",
    "            })\n",
    "\n",
    "    elif sample_id == 1:\n",
    "        # sample 1 → test sample\n",
    "        cutoff = int(total * TEST_RATIO)\n",
    "        selected = sents[:cutoff]\n",
    "\n",
    "        for i, text in enumerate(selected):\n",
    "            test_items.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"sentence_id\": i,\n",
    "                \"text\": text,\n",
    "                \"annotation\": []\n",
    "            })\n",
    "\n",
    "print(\"Sample 0 sentences:\", len([s for s in nlp(df.loc[0, \"text\"]).sents]))\n",
    "print(\"Sample 1 sentences:\", len([s for s in nlp(df.loc[1, \"text\"]).sents]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d519d",
   "metadata": {},
   "source": [
    "### 3.1 Manual Evaluation of Standard NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf3736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test Text Preview ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ADMISSION\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " DIAGNOSES:,1.  Atypical chest pain.,2.  Nausea.,3.  \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Vomiting\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".,4.  Diabetes.,5.  \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Hypokalemia\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".,6.  \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Diarrhea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".,7.  Panic and depression.,8.  Hypertension.,DISCHARGE DIAGNOSES:,1.  Serotonin syndrome secondary to high doses of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Prozac\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".,2.  Atypical chest pain with myocardial infarction ruled out.,3.  Diabetes mellitus.,4.  Hypertension.,5.  \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Diarrhea\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " resolved.,ADMISSION SUMMARY: , The patient is a \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    53-year-old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " woman with history of hypertension, diabetes, and depression.  Unfortunately her husband left her \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    10 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " prior to admission and she developed severe anxiety and depression.  She was having chest pains along with significant vomiting and diarrhea.  Of note, she had a nuclear stress test performed in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    February of this year\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", which was normal.  She was readmitted to the hospital to rule out myocardial infarction and for further evaluation.,ADMISSION PHYSICAL: , Significant for her being afebrile.  Apparently there was \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    one\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " temperature registered mildly high at \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ".  Her blood pressure was \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    140/82\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", heart rate \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    83\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", oxygen saturation was \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       ".  She was tearful.  HEART:  Heart sounds were regular.  LUNGS:  Clear.  ABDOMEN:  \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Soft\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ".  Apparently there were some level of restlessness and acathexia.  She was also pacing.,ADMISSION LABS:  ,Showed \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CBC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " with a white count of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    16.9\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", hematocrit of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    46.9\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", platelets \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    318,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ".  She had \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    80%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " neutrophils, no bands.  \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    UA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    05/02\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " came out negative.  Chemistry panel shows sodium \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    138\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", potassium \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    3.5\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", creatinine \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    0.6\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", calcium \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    8.3\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", lactate \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    0.9\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ", ALT was \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    39\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", AST 38, total bilirubin \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    0.6\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ".  Her initial CK came out at \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    922\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ".  CK-MB was low.  \n",
       "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Troponin\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " was \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    0.04\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       ".  She had a normal amylase and lipase.  Previous TSH few days prior was normal.  Chest x-ray was negative.,HOSPITAL COURSE:,1.  Serotonin syndrome.  After reevaluation of the patient including evaluation of the lab abnormalities it was felt that she likely had serotonin syndrome with obvious restlessness, increased bowel activity, agitation, and elevated white count and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CPK\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".  She did not have fever, </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Manual Evaluation of Standard NER\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "\n",
    "# 1. Load sample text file\n",
    "df_samples = pd.read_csv(PATH_TO_ANNOTATIONS + \"sample_for_annotation.csv\")\n",
    "\n",
    "# 2. Extract second row text\n",
    "# sample_for_annotation.csv has:\n",
    "# row 0: header \"text\"\n",
    "# row 1: sample 1 -> this is the train sample for further NER extention \n",
    "# row 2: sample 2  -> this is the test sample\n",
    "test_text = df_samples.iloc[1][\"text\"]\n",
    "\n",
    "print(\"=== Test Text Preview ===\")\n",
    "\n",
    "# 3. Load spaCy model (baseline or your updated one)\n",
    "nlp = spacy.load(\"en_core_web_md\")   # or: spacy.load(\"output_medical_ner\")\n",
    "\n",
    "# 4. Run NER on the full document\n",
    "doc = nlp(test_text[:2000])\n",
    "\n",
    "# 5. Render using displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24f233",
   "metadata": {},
   "source": [
    "Observation: Some NERs don't make sense. E.g., Vomitting -> PERSON, Hypokalemia -> PERSON, Diarrhea -> PERSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c109e22",
   "metadata": {},
   "source": [
    "### 3.2 Automatic Evaluation of Standard NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f0360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 annotated sentences\n",
      "\n",
      "===== Baseline spaCy NER Evaluation =====\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 score:  0.0000\n",
      "\n",
      "Macro Precision: 0.0\n",
      "Macro Recall:    0.0\n",
      "Macro F1:        0.0\n",
      "\n",
      "===== Examples of WRONG predictions =====\n",
      "\n",
      "Text: ADMISSION DIAGNOSES:,1.\n",
      "Gold: set()\n",
      "Pred: {(0, 9, 'ORG')}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  Standard NER evaluation\n",
    "# =========================================\n",
    "\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from ner.utils import extract_spans, convert_to_labels\n",
    "\n",
    "# 1. Load JSONL annotations\n",
    "jsonl_path = PATH_TO_ANNOTATIONS + \"test_annotation_complete.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        data.append(obj)\n",
    "\n",
    "print(f\"Loaded {len(data)} annotated sentences\")\n",
    "\n",
    "# 2. Load baseline spaCy NER\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# 3. Convert annotations to character-level spans\n",
    "gold_spans, pred_spans, labels = extract_spans(nlp, data)\n",
    "\n",
    "# 4. Convert spans to entity sets for evaluation\n",
    "true_labels, pred_labels = convert_to_labels(gold_spans, pred_spans)\n",
    "\n",
    "# 5. Evaluate macro and micro F1\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n===== Baseline spaCy NER Evaluation =====\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 score:  {f1:.4f}\")\n",
    "\n",
    "prec_m, rec_m, f1_m, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nMacro Precision:\", round(prec_m, 4))\n",
    "print(\"Macro Recall:   \", round(rec_m, 4))\n",
    "print(\"Macro F1:       \", round(f1_m, 4))\n",
    "\n",
    "# 6. Show some error cases\n",
    "print(\"\\n===== Examples of WRONG predictions =====\\n\")\n",
    "\n",
    "for i, (g, p, item) in enumerate(zip(gold_spans, pred_spans, data)):\n",
    "    g_set = set(g)\n",
    "    p_set = set(p)\n",
    "    if g_set != p_set:\n",
    "        print(\"Text:\", item[\"text\"])\n",
    "        print(\"Gold:\", g_set)\n",
    "        print(\"Pred:\", p_set)\n",
    "        print(\"-\" * 50)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26c084",
   "metadata": {},
   "source": [
    "## 4. Extend the standard NER types using the NER Annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b2181",
   "metadata": {},
   "source": [
    "### 4.1 Training Extended NER model with >100 manual annoatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1e19587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Overlapping entity problems found =====\n"
     ]
    }
   ],
   "source": [
    "# Make sure there is no overlapping entity problems in the training sample\n",
    "# \n",
    "# import json\n",
    "\n",
    "path = PATH_TO_ANNOTATIONS + \"train_annotation_complete.jsonl\"\n",
    "\n",
    "def spans_overlap(a, b):\n",
    "    return not (a[1] <= b[0] or b[1] <= a[0])\n",
    "\n",
    "bad = []\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"]\n",
    "        anns = obj[\"annotation\"]  # [[phrase,label],...]\n",
    "\n",
    "        spans = []\n",
    "        for phrase, label in anns:\n",
    "            start = text.lower().find(phrase.lower())\n",
    "            if start == -1:\n",
    "                continue\n",
    "            end = start + len(phrase)\n",
    "            spans.append((start, end, phrase, label))\n",
    "\n",
    "        # check overlapping\n",
    "        for i in range(len(spans)):\n",
    "            for j in range(i+1, len(spans)):\n",
    "                a = spans[i]\n",
    "                b = spans[j]\n",
    "                if spans_overlap(a, b):\n",
    "                    bad.append((obj[\"sentence_id\"], text, a, b))\n",
    "\n",
    "print(\"===== Overlapping entity problems found =====\")\n",
    "for sid, text, a, b in bad:\n",
    "    print(f\"\\nSentence {sid}: {text}\")\n",
    "    print(\"  ->\", a)\n",
    "    print(\"  ->\", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d44df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 83 annotated sentences\n",
      "Custom NER labels: ['DISEASE', 'BODY_PART', 'FINDING', 'PROCEDURE', 'SYMPTOM']\n",
      "Example: ('REASON FOR THE CONSULT:,  Nonhealing right ankle stasis ulcer.,HISTORY', {'entities': [(26, 36, 'FINDING'), (37, 48, 'BODY_PART'), (49, 61, 'DISEASE')]})\n",
      "Epoch 1/35 Loss: {'ner': np.float32(776.60944)}\n",
      "Epoch 2/35 Loss: {'ner': np.float32(269.15738)}\n",
      "Epoch 3/35 Loss: {'ner': np.float32(131.04156)}\n",
      "Epoch 4/35 Loss: {'ner': np.float32(118.32787)}\n",
      "Epoch 5/35 Loss: {'ner': np.float32(100.092255)}\n",
      "Epoch 6/35 Loss: {'ner': np.float32(96.86833)}\n",
      "Epoch 7/35 Loss: {'ner': np.float32(96.317795)}\n",
      "Epoch 8/35 Loss: {'ner': np.float32(74.446815)}\n",
      "Epoch 9/35 Loss: {'ner': np.float32(68.70432)}\n",
      "Epoch 10/35 Loss: {'ner': np.float32(67.18328)}\n",
      "Epoch 11/35 Loss: {'ner': np.float32(66.88499)}\n",
      "Epoch 12/35 Loss: {'ner': np.float32(61.775448)}\n",
      "Epoch 13/35 Loss: {'ner': np.float32(80.03961)}\n",
      "Epoch 14/35 Loss: {'ner': np.float32(73.56988)}\n",
      "Epoch 15/35 Loss: {'ner': np.float32(84.17685)}\n",
      "Epoch 16/35 Loss: {'ner': np.float32(68.1092)}\n",
      "Epoch 17/35 Loss: {'ner': np.float32(52.224678)}\n",
      "Epoch 18/35 Loss: {'ner': np.float32(43.332996)}\n",
      "Epoch 19/35 Loss: {'ner': np.float32(48.516228)}\n",
      "Epoch 20/35 Loss: {'ner': np.float32(43.411556)}\n",
      "Epoch 21/35 Loss: {'ner': np.float32(30.363493)}\n",
      "Epoch 22/35 Loss: {'ner': np.float32(35.273045)}\n",
      "Epoch 23/35 Loss: {'ner': np.float32(20.514793)}\n",
      "Epoch 24/35 Loss: {'ner': np.float32(20.210035)}\n",
      "Epoch 25/35 Loss: {'ner': np.float32(13.936901)}\n",
      "Epoch 26/35 Loss: {'ner': np.float32(11.792618)}\n",
      "Epoch 27/35 Loss: {'ner': np.float32(7.511404)}\n",
      "Epoch 28/35 Loss: {'ner': np.float32(7.412055)}\n",
      "Epoch 29/35 Loss: {'ner': np.float32(7.033544)}\n",
      "Epoch 30/35 Loss: {'ner': np.float32(8.810991)}\n",
      "Epoch 31/35 Loss: {'ner': np.float32(1.2566657)}\n",
      "Epoch 32/35 Loss: {'ner': np.float32(3.210168)}\n",
      "Epoch 33/35 Loss: {'ner': np.float32(7.1811824)}\n",
      "Epoch 34/35 Loss: {'ner': np.float32(2.129098)}\n",
      "Epoch 35/35 Loss: {'ner': np.float32(3.1842208)}\n",
      "Model saved to output_medical_ner\n",
      "\n",
      "Test sentence: He is also a former cigarette smoker, quit several years ago.\n",
      "Predicted NER: []\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  Extended NER TRAINING\n",
    "# =========================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "from ner.utils import find_span\n",
    "\n",
    "# 1. Load annotated JSONL\n",
    "json_path = PATH_TO_ANNOTATIONS + \"train_annotation_complete.jsonl\"\n",
    "data = []\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(data)} annotated sentences\")\n",
    "\n",
    "# 2. Custom labels\n",
    "CUSTOM_LABELS = [\"DISEASE\", \"BODY_PART\", \"FINDING\", \"PROCEDURE\", \"SYMPTOM\"]\n",
    "print(\"Custom NER labels:\", CUSTOM_LABELS)\n",
    "\n",
    "# 3. Prepare training data\n",
    "training_examples = []\n",
    "\n",
    "for item in data:\n",
    "    text = item[\"text\"]\n",
    "    labels = item[\"annotation\"]  # [[\"hypertension\",\"DISEASE\"], ...]\n",
    "\n",
    "    entities = []\n",
    "    for phrase, label in labels:\n",
    "        span = find_span(text, phrase)\n",
    "        if span:\n",
    "            entities.append((span[0], span[1], label))\n",
    "\n",
    "    training_examples.append((text, {\"entities\": entities}))\n",
    "\n",
    "print(\"Example:\", training_examples[0])\n",
    "\n",
    "# 4. Initialize blank model for spaCy 3.8+\n",
    "nlp = spacy.blank(\"en\")         \n",
    "ner = nlp.add_pipe(\"ner\")       \n",
    "\n",
    "# Add custom labels\n",
    "for label in CUSTOM_LABELS:\n",
    "    ner.add_label(label)\n",
    "\n",
    "# 5. Training loop\n",
    "n_iter = 35\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "    random.shuffle(training_examples)\n",
    "    losses = {}\n",
    "\n",
    "    batches = minibatch(training_examples, size=compounding(4.0, 32.0, 1.5))\n",
    "\n",
    "    for batch in batches:\n",
    "        examples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in batch]\n",
    "        nlp.update(examples, sgd=optimizer, drop=0.2, losses=losses)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_iter} Loss: {losses}\")\n",
    "\n",
    "# 6. Save model\n",
    "output_dir = \"output_medical_ner\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Model saved to\", output_dir)\n",
    "\n",
    "# 7. Quick sanity check\n",
    "test_text = random.choice(data)[\"text\"]\n",
    "doc = nlp(test_text)\n",
    "\n",
    "print(\"\\nTest sentence:\", test_text)\n",
    "print(\"Predicted NER:\", [(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229beae",
   "metadata": {},
   "source": [
    "### 4.2 Extended NER evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36d2a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 annotated sentences\n",
      "\n",
      "===== Extended spaCy NER Evaluation =====\n",
      "Precision: 0.1515\n",
      "Recall:    0.1515\n",
      "F1 score:  0.1515\n",
      "\n",
      "Macro Precision: 0.25\n",
      "Macro Recall:    0.0734\n",
      "Macro F1:        0.1111\n",
      "\n",
      "===== Examples of WRONG predictions =====\n",
      "\n",
      "Text: Nausea.,3.\n",
      "Gold: {(0, 6, 'SYMPTOM')}\n",
      "Pred: set()\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  Extended NER evaluation with test sample\n",
    "# =========================================\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from ner.utils import extract_spans, convert_to_labels\n",
    "\n",
    "# 1. Load JSONL annotations for test\n",
    "jsonl_path = PATH_TO_ANNOTATIONS + \"test_annotation_complete.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        data.append(obj)\n",
    "\n",
    "print(f\"Loaded {len(data)} annotated sentences\")\n",
    "\n",
    "# 2. Load Extended spaCy NER\n",
    "nlp = spacy.load(\"output_medical_ner\")\n",
    "\n",
    "# 3. Convert annotations to character-level spans\n",
    "gold_spans, pred_spans, labels = extract_spans(nlp, data)\n",
    "\n",
    "# 4. Convert spans to entity sets for evaluation\n",
    "true_labels, pred_labels = convert_to_labels(gold_spans, pred_spans)\n",
    "# 5. Evaluate macro and micro F1\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n===== Extended spaCy NER Evaluation =====\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 score:  {f1:.4f}\")\n",
    "\n",
    "prec_m, rec_m, f1_m, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nMacro Precision:\", round(prec_m, 4))\n",
    "print(\"Macro Recall:   \", round(rec_m, 4))\n",
    "print(\"Macro F1:       \", round(f1_m, 4))\n",
    "\n",
    "# 6. Show some error cases\n",
    "print(\"\\n===== Examples of WRONG predictions =====\\n\")\n",
    "\n",
    "for i, (g, p, item) in enumerate(zip(gold_spans, pred_spans, data)):\n",
    "    g_set = set(g)\n",
    "    p_set = set(p)\n",
    "    if g_set != p_set:\n",
    "        print(\"Text:\", item[\"text\"])\n",
    "        print(\"Gold:\", g_set)\n",
    "        print(\"Pred:\", p_set)\n",
    "        print(\"-\" * 50)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a41efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfa558d",
   "metadata": {},
   "source": [
    "## 5. LLM-based NER classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "#  Extended NER evaluation with test sample\n",
    "# =========================================\n",
    "import json\n",
    "import spacy\n",
    "import time\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from spacy_llm.util import assemble\n",
    "\n",
    "# ===================================================\n",
    "# 1. Load JSONL annotations for test\n",
    "# ===================================================\n",
    "jsonl_path = \"test_annotation_complete.jsonl\"\n",
    "\n",
    "data = []\n",
    "with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        data.append(obj)\n",
    "\n",
    "print(f\"Loaded {len(data)} annotated sentences\")\n",
    "\n",
    "# ===================================================\n",
    "# 2. Create LLM NER\n",
    "# ===================================================\n",
    "\n",
    "nlp = assemble(\"config.cfg\")\n",
    "print(\"spaCy model loaded:\", nlp.meta[\"name\"])\n",
    "\n",
    "# ===================================================\n",
    "# 3. Convert annotations to character-level spans\n",
    "# ===================================================\n",
    "\n",
    "def find_span(text, phrase):\n",
    "    \"\"\"\n",
    "    Locate (start, end) in the sentence.\n",
    "    If multiple choices, take the first one.\n",
    "    Retrun None if not found。\n",
    "    \"\"\"\n",
    "    start = text.lower().find(phrase.lower())\n",
    "    if start == -1:\n",
    "        return None\n",
    "    return (start, start + len(phrase))\n",
    "\n",
    "gold_spans = []\n",
    "pred_spans = []\n",
    "labels = []   # For all ground-truth annotion in the test\n",
    "\n",
    "print(\"Creating docs\")\n",
    "for item in data:\n",
    "    text = item[\"text\"]\n",
    "    ann_list = item[\"annotation\"]  # [[\"ankle\",\"BODY_PART\"], ...]\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    print('.')\n",
    "    time.sleep(2) \n",
    "\n",
    "    # -------- gold spans --------\n",
    "    gold = []\n",
    "    for phrase, label in ann_list:\n",
    "        span = find_span(text, phrase)\n",
    "        if span is not None:\n",
    "            gold.append((span[0], span[1], label))\n",
    "            labels.append(label)\n",
    "        # else:\n",
    "        #     print(\"Warning: phrase not found:\", phrase)\n",
    "\n",
    "    gold_spans.append(gold)\n",
    "\n",
    "    # -------- predicted spans --------\n",
    "    pred = []\n",
    "    for ent in doc.ents:\n",
    "        pred.append((ent.start_char, ent.end_char, ent.label_))\n",
    "\n",
    "    pred_spans.append(pred)\n",
    "\n",
    "# ===================================================\n",
    "# 4. Convert spans to entity sets for evaluation\n",
    "# ===================================================\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for g_spans, p_spans in zip(gold_spans, pred_spans):\n",
    "\n",
    "    #  (start,end,label) precise matching\n",
    "    g_set = set(g_spans)\n",
    "    p_set = set(p_spans)\n",
    "\n",
    "    # True positives\n",
    "    for span in g_set:\n",
    "        if span in p_set:\n",
    "            true_labels.append(span[2])\n",
    "            pred_labels.append(span[2])\n",
    "        else:\n",
    "            true_labels.append(span[2])\n",
    "            pred_labels.append(\"NONE\")  # missed\n",
    "\n",
    "    # False positives\n",
    "    for span in p_set:\n",
    "        if span not in g_set:\n",
    "            true_labels.append(\"NONE\")     # if no gold annotation\n",
    "            pred_labels.append(span[2])    # wrong\n",
    "\n",
    "# ===================================================\n",
    "# 5. Evaluate macro and micro F1\n",
    "# ===================================================\n",
    "\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n===== Extended spaCy NER Evaluation =====\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 score:  {f1:.4f}\")\n",
    "\n",
    "prec_m, rec_m, f1_m, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nMacro Precision:\", round(prec_m, 4))\n",
    "print(\"Macro Recall:   \", round(rec_m, 4))\n",
    "print(\"Macro F1:       \", round(f1_m, 4))\n",
    "\n",
    "\n",
    "# ===================================================\n",
    "# 6. Show some error cases\n",
    "# ===================================================\n",
    "print(\"\\n===== Examples of WRONG predictions =====\\n\")\n",
    "\n",
    "for i, (g, p, item) in enumerate(zip(gold_spans, pred_spans, data)):\n",
    "    g_set = set(g)\n",
    "    p_set = set(p)\n",
    "    if g_set != p_set:\n",
    "        print(\"Text:\", item[\"text\"])\n",
    "        print(\"Gold:\", g_set)\n",
    "        print(\"Pred:\", p_set)\n",
    "        print(\"-\" * 50)\n",
    "        break\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
