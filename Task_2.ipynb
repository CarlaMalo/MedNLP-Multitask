{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2340435",
   "metadata": {},
   "source": [
    "# Task 2: Data Exploration and Processing\n",
    "\n",
    "## 1. Manual data inspection\n",
    "- Investigate which standard and potential new NER types are most prominent in your data set (i.e., manual data inspection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45545326",
   "metadata": {},
   "source": [
    "Following visual inspection, some of the prominent NERs found are: DATE, BODY_PART, DOSAGE, MEASUREMENT, DRUG and SYMPTOM. \n",
    "\n",
    "Examples:\n",
    " 1. DATE: 12/20/2005, 1/19/96\n",
    " 2. BODY_PART: nose, abdomen, knee\n",
    " 3. DOSAGE:  10/40 mg one a day, 0.25 micrograms a day, 50 mg twice a day, 10 ml\n",
    " 4. MEASUREMENT: 3.98 kg, 8mm, pulse of 84, blood pressure 108/65\n",
    " 5. DRUG:  Vytorin, Rocaltrol, Carvedilol, Cozaar,  Lasix\n",
    " 6. SYMPTOM: erythematous, chest pain, constipated\n",
    "\n",
    "Where, DATE is a **standard NER in spacy** and the remaining ones fall in the medicine domain category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cf5702",
   "metadata": {},
   "source": [
    "## 2. Apply the standard NER classifier of spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4cb76f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4127fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "# import utils.annotations_utils as utils_ann # ChatGPT API called to identify keywords/phrases associated with entities\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af4584a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f457d37",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "392028f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features available\n",
      "['text', 'inputs', 'prediction', 'prediction_agent', 'annotation', 'annotation_agent', 'multi_label', 'explanation', 'id', 'metadata', 'status', 'event_timestamp', 'metrics']\n",
      "Format of 'prediction' column\n",
      "List({'label': Value('string'), 'score': Value('float64')})\n",
      "Dataset length:  4966\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# 1: Load Dataset\n",
    "# ============================\n",
    "dataset = load_dataset(\"argilla/medical-domain\", split=\"train\")\n",
    "print(\"Features available\")\n",
    "print(dataset.column_names)\n",
    "print(\"Format of 'prediction' column\")\n",
    "print(dataset.features['prediction'])\n",
    "print(\"Dataset length: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc5f52b",
   "metadata": {},
   "source": [
    "### Filter such that we keep only samples pertaining to 'Surgery'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a8beaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 4966/4966 [00:00<00:00, 20966.75 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_slimmed = dataset.filter(\n",
    "\tlambda row: (\n",
    "\t\tisinstance(row[\"text\"], str)\n",
    "        and row[\"text\"] != ''\n",
    "\t\tand 'Surgery' in str(row[\"prediction\"][0])\n",
    "\t)\n",
    ")\n",
    "dataset = dataset_slimmed\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad783949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our NER schema =  ['SYMPTOM', 'BODY_PART', 'DISEASE', 'DRUG', 'ROUTE']\n"
     ]
    }
   ],
   "source": [
    "# ================================================\n",
    "# 2: Define Entity Schema\n",
    "# ================================================\n",
    "# Our final gold-label schema\n",
    "CUSTOM_LABELS = ['SYMPTOM', 'BODY_PART', 'DISEASE', 'DRUG', 'ROUTE']\n",
    "\n",
    "print(\"Our NER schema = \", CUSTOM_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98f45d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved unannotated_samples.csv with 4 sentences\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# 3: Pick N samples manually\n",
    "# =========================================\n",
    "PATH_TO_ANNOTATIONS = \"ner/samples/\"\n",
    "SEED = 42 \n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "N_SAMPLES = 4   # select a proper number to contain 100 ground-truth NERs \n",
    "sampled_texts = []\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "\trow = dataset[np.random.randint(0, len(dataset))]\n",
    "\ttext = row[\"text\"]\n",
    "\tsampled_texts.append(text)\n",
    "\n",
    "df_samples = pd.DataFrame({\"text\": sampled_texts})\n",
    "df_samples.to_csv(PATH_TO_ANNOTATIONS + \"unannotated_samples.csv\", index=False)\n",
    "\n",
    "print(\"Saved unannotated_samples.csv with\", len(df_samples), \"sentences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9c1289",
   "metadata": {},
   "source": [
    "## 3. Evaluation of Standard NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc35f24",
   "metadata": {},
   "source": [
    "### Perform manual annotations, then split into train/test \n",
    "\n",
    " The annotation format is: \n",
    "\n",
    "\t{\n",
    "\t\t\"classes\": List(labels), \n",
    "\t\t\"annotations\": List(\n",
    "\t\t\t[str(text), dict(\"entities\": List(List(start idx, end idx, class)))]\n",
    "\t\t)\n",
    "\t}\n",
    "Where each item in \"annotations\" is a sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78713c24",
   "metadata": {},
   "source": [
    "#### Manual Annotations (with AI assistance)\n",
    "\n",
    "**NOTE: Skip this section if unannotated_samples.csv, annotatated_samples\\*.json exist**\n",
    "\n",
    "Jump to [Train/Test splitting](#traintest-split-of-annotations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ba636a6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "unannotated_samples_path = PATH_TO_ANNOTATIONS + \"unannotated_samples.csv\"\n",
    "df = pd.read_csv(unannotated_samples_path, header=0)\n",
    "text = df['text'].to_list()\n",
    "num_examples_per_label = 30\n",
    "annotations = utils_ann.chatgpt_annotate_text(CUSTOM_LABELS, text, num_examples_per_label)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7561c64",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "for label in annotations.keys():\n",
    "\tprint(f\" Label: {label} \".center(30, '='))\n",
    "\tfor indexed_words in annotations[label]:\n",
    "\t\tprint(indexed_words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8949bb17",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Specify examples to exclude or modify\n",
    "# NOTE: pop larger indices first to maintain index ordering of earlier items after deletion\n",
    "annotations['SYMPTOM'].pop(29)\n",
    "\n",
    "annotations['BODY_PART'][8][1] = 'joint'\n",
    "annotations['BODY_PART'][14][1] = 'limb'\n",
    "annotations['BODY_PART'].pop(17)\n",
    "\n",
    "annotations['DISEASE'][0][1] = 'renal disease'\n",
    "\n",
    "annotations['DRUG'][3][1] = 'anesthetics'\n",
    "annotations['DRUG'][9][1] = 'saline'\n",
    "annotations['DRUG'].pop(28)\n",
    "\n",
    "annotations['ROUTE'].pop(17)\n",
    "annotations['ROUTE'].pop(6)\n",
    "\n",
    "for label in annotations.keys():\n",
    "\tprint(f\" Label: {label} \".center(30, '='))\n",
    "\twords_list = []\n",
    "\tfor idx, word in annotations[label]:\n",
    "\t\twords_list.append(word)\n",
    "\t\tprint(word)\n",
    "\tannotations[label] = words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d8646",
   "metadata": {},
   "source": [
    "#### Save annotations to the appropriate format"
   ]
  },
  {
   "cell_type": "raw",
   "id": "915cb7be",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 1. Load samples: only column \"text\"\n",
    "df = pd.read_csv(unannotated_samples_path)\n",
    "\n",
    "# 2. Load spaCy for sentence splitting\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "if \"sentencizer\" not in nlp.pipe_names:\n",
    "\tnlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# 3. Extract all sentences across samples\n",
    "all_sentences = []\n",
    "for _, row in df.iterrows():\n",
    "\tfull_text = str(row[\"text\"])\n",
    "\tdoc = nlp(full_text)\n",
    "\n",
    "\t# Extract sentences\n",
    "\tall_sentences.extend([s.text.strip() for s in doc.sents if len(s.text.strip()) > 0])\n",
    "\n",
    "# 4. Annotate all identified keywords in sentences and save to json file\n",
    "utils_ann.annotate_sentences_and_save(all_sentences, annotations, CUSTOM_LABELS, PATH_TO_ANNOTATIONS+'annotated_samples.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175dfcb8",
   "metadata": {},
   "source": [
    "### Train/Test split of annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c697b5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Train samples:  84\n",
      "N Test samples:  36\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Generate train/test annotation template JSONL with ratio control\n",
    "# ============================================\n",
    "annotated_samples_path = PATH_TO_ANNOTATIONS + \"annotated_samples.json\"\n",
    "\n",
    "# =====================================================\n",
    "# Parameters: control split ratio (sentence-wise split)\n",
    "# =====================================================\n",
    "TRAIN_RATIO = 0.70     # first 70% for training\n",
    "TEST_RATIO  = 0.30     # first 30% for evaluation/test\n",
    "\n",
    "# ================================\n",
    "\n",
    "# Load annotated samples: only key \"annotations\"\n",
    "annotations_dict = None\n",
    "with open(annotated_samples_path) as annotations_file:\n",
    "\tannotations_dict = json.load(annotations_file)\n",
    "all_sentences = annotations_dict[\"annotations\"]\n",
    "\n",
    "# ===========================\n",
    "# Create train/test sets\n",
    "# ===========================\n",
    "\n",
    "np.random.shuffle(all_sentences) # shuffle randomly first before splitting\n",
    "total = len(all_sentences)\n",
    "train_cutoff = int(total * TRAIN_RATIO)\n",
    "test_cutoff = train_cutoff + int(total * TEST_RATIO)\n",
    "\n",
    "train_sentences = all_sentences[:train_cutoff]\n",
    "test_sentences = all_sentences[train_cutoff:test_cutoff]\n",
    "\n",
    "print(\"N Train samples: \", len(train_sentences))\n",
    "print(\"N Test samples: \", len(test_sentences))\n",
    "# ===========================\n",
    "# Save sentences into train/test\n",
    "# ===========================\n",
    "train_samples_path = PATH_TO_ANNOTATIONS + 'annotated_samples_train.json'\n",
    "annotations_dict['annotations'] = train_sentences\n",
    "with open(train_samples_path, 'w') as fp:\n",
    "\tjson.dump(annotations_dict, fp)\n",
    "\n",
    "test_samples_path = PATH_TO_ANNOTATIONS + 'annotated_samples_test.json'\n",
    "annotations_dict['annotations'] = test_sentences\n",
    "with open(test_samples_path, 'w') as fp:\n",
    "\tjson.dump(annotations_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3d519d",
   "metadata": {},
   "source": [
    "### 3.1 Manual Evaluation of Standard NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bf3736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.3/33.5 MB 7.5 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 3.1/33.5 MB 8.0 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 4.7/33.5 MB 7.9 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 6.6/33.5 MB 8.1 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 8.1/33.5 MB 7.9 MB/s eta 0:00:04\n",
      "     ----------- ---------------------------- 9.4/33.5 MB 7.6 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 10.7/33.5 MB 7.5 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 12.3/33.5 MB 7.4 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 13.6/33.5 MB 7.4 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 14.9/33.5 MB 7.3 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 16.3/33.5 MB 7.1 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 17.6/33.5 MB 7.1 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 19.4/33.5 MB 7.2 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 21.0/33.5 MB 7.2 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 22.5/33.5 MB 7.2 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 23.9/33.5 MB 7.2 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 25.4/33.5 MB 7.2 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 27.8/33.5 MB 7.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 29.4/33.5 MB 7.4 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 30.9/33.5 MB 7.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 32.5/33.5 MB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 33.5/33.5 MB 7.3 MB/s  0:00:04\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "=== Test Text Preview ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">All sponges were counted encountered for as were sutures.This was sutured using \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       "-0The tissues were stretched with tissue scissors and then a high speed instrumentation was used to decorticate the anterior mandible using a \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1.6 mm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " twist drill and a pear shaped bur was used in the posterior region to begin original exploratory phenomenon of repair.He is to follow up in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the next 10 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " for a check.,PROCEDURE: , Bilateral Crawford subtalar arthrodesis with open \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Achilles Z\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "-lengthening and bilateral long-leg cast.X and company accompanied the patient to OR #\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " at 7:30 a.m.1% \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Xylocaine\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1:100,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " epinephrine was infiltrated in the labial mucosa 5 cc were given.The hard palate was directly observed.Once the foot was reduced a \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steinman\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " pin was used to hold it in position.The patient was subsequently was taken to Recovery in stable condition.The contents of the neurovascular canal from the greater \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    palatine\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " foramina were identified.The wound was irrigated with normal saline.Release incisions were made in the posterior region of the maxilla.Particulate bone was then injected into the posterior tunnels bilaterally.Bilateral long-leg casts were then placed with the foot in neutral with some moulding of his medial plantar arch.Acquired facial deformity.Bilateral nonsterile tourniquets were placed on each thigh.,SPECIMENS: , None.,HARDWARE,POSTOPERATIVE PLAN: , The patient will be hospitalized \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    overnight\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " for pain as per parents' request.PREOPERATIVE DIAGNOSES:,1.A block of bone was inserted between the mental foramina and fixative with \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    three 16 cm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " screws \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " with a twist drill then followed with self-tapping \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2 mm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " diameter titanium screws.The ankle was taken through a range of motion with noted improvement in the reduction of the talocalcaneal alignment with the foot in plantar flexion on the lateral view.All questions were answered and the mother agreed to the above plan.,PROCEDUREA similar procedure was done on the contralateral side.The estimated blood loss in the intraoral procedure was \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    220\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " cc.  Total blood loss for the procedure \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    320\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " cc.DIAGNOSIS,End-stage renal disease.The wound was cleaned and dried, dressed with \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Steri-Strips\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Xeroform\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", and \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " x \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    4s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Webril\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".USED: , \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Staple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    7/8 inch\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " x1 on each side.,PROCEDURE,\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Venogram\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " of the left arm and creation of left brachiocephalic arteriovenous fistula.He has been having significant feet pain with significant planovalgus deformity.A primary incision was made between the mental foramina and the residual crest of the ridge and reflected \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " to the lingual area observing the superior genial tubercle in the facial area degloving the mentalis muscle and exposing the anterior body.A piece of \n",
       "<mark class=\"entity\" style=\"background: #bfeeb7; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AlloDerm\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PRODUCT</span>\n",
       "</mark>\n",
       " mixed with \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Croften\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and patient's platelet-rich plasma, which was centrifuged from drawing \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    20\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " cc of blood was then mixed together and placed over the lateral aspect of the block.A primary incision was made in the maxilla starting on the patient's left tuberosity region along the crest of the residual ridge to the contralateral side in similar fashion.The urine out 180.,3.Xylocaine \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1:100,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " epinephrine \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    7 ml\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">QUANTITY</span>\n",
       "</mark>\n",
       " was infiltrated into the labial and palatal mucosa.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Manual Evaluation of Standard NER\n",
    "from spacy import displacy\n",
    "!python -m spacy download en_core_web_md\n",
    "# If we jump here directly, we need a definition of test_samples_path\n",
    "test_samples_path = PATH_TO_ANNOTATIONS + 'annotated_samples_test.json'\n",
    "\n",
    "# 1. Load sample text\n",
    "annotations_dict = None\n",
    "with open(test_samples_path) as annotations_file:\n",
    "\tannotations_dict = json.load(annotations_file)\n",
    "\t\n",
    "# 2. Extract second row text\n",
    "test_text = [sent for sent, _ in annotations_dict[\"annotations\"]]\n",
    "\n",
    "print(\"=== Test Text Preview ===\")\n",
    "\n",
    "# 3. Load spaCy model (baseline or your updated one)\n",
    "nlp = spacy.load(\"en_core_web_md\")   # or: spacy.load(\"output_medical_ner\")\n",
    "\n",
    "# 4. Run NER on the full document\n",
    "doc = nlp(''.join(test_text[:2000]))\n",
    "\n",
    "# 5. Render using displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24f233",
   "metadata": {},
   "source": [
    "Observation: Some NERs don't make sense. E.g., Vomitting -> PERSON, Hypokalemia -> PERSON, Diarrhea -> PERSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c109e22",
   "metadata": {},
   "source": [
    "### 3.2 Automatic Evaluation of Standard NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f0360a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 36 annotated sentences\n",
      "True labels\n",
      " ['DRUG', 'DRUG', 'NONE', 'BODY_PART', 'NONE', 'NONE', 'BODY_PART', 'NONE', 'NONE', 'NONE', 'DRUG', 'DRUG', 'NONE', 'NONE', 'BODY_PART', 'BODY_PART', 'NONE', 'NONE', 'DRUG', 'BODY_PART', 'BODY_PART', 'BODY_PART', 'BODY_PART', 'DISEASE', 'BODY_PART', 'SYMPTOM', 'NONE', 'BODY_PART', 'BODY_PART', 'NONE', 'NONE', 'NONE', 'BODY_PART', 'ROUTE', 'NONE', 'NONE', 'DISEASE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'ROUTE', 'NONE', 'SYMPTOM', 'DISEASE', 'SYMPTOM', 'BODY_PART', 'BODY_PART', 'BODY_PART', 'NONE', 'DRUG', 'NONE', 'NONE', 'BODY_PART', 'NONE', 'NONE', 'DRUG', 'DRUG', 'NONE', 'NONE', 'NONE']\n",
      "pred\n",
      " ['NONE', 'NONE', 'CARDINAL', 'NONE', 'QUANTITY', 'DATE', 'NONE', 'PERSON', 'MONEY', 'TIME', 'PERSON', 'NONE', 'PERCENT', 'CARDINAL', 'NONE', 'NONE', 'PERSON', 'GPE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'TIME', 'NONE', 'NONE', 'QUANTITY', 'ORDINAL', 'QUANTITY', 'NONE', 'NONE', 'CARDINAL', 'CARDINAL', 'NONE', 'ORG', 'GPE', 'CARDINAL', 'CARDINAL', 'DATE', 'ORG', 'QUANTITY', 'NONE', 'ORG', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'ORDINAL', 'PRODUCT', 'PERSON', 'CARDINAL', 'NONE', 'CARDINAL', 'CARDINAL', 'PERSON', 'NONE', 'PERCENT', 'CARDINAL', 'QUANTITY']\n",
      "sent ids\n",
      " [0, 1, 1, 2, 2, 3, 4, 4, 5, 5, 6, 6, 6, 6, 7, 8, 8, 10, 11, 12, 13, 14, 14, 15, 16, 18, 18, 20, 20, 20, 20, 20, 21, 24, 24, 24, 25, 26, 26, 26, 26, 26, 27, 27, 28, 28, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 32, 33, 34, 35, 35, 35, 35, 35]\n",
      "\n",
      "===== Baseline spaCy NER Evaluation =====\n",
      "Precision: 0.0000\n",
      "Recall:    0.0000\n",
      "F1 score:  0.0000\n",
      "\n",
      "Macro Precision: 0.0\n",
      "Macro Recall:    0.0\n",
      "Macro F1:        0.0\n",
      "\n",
      "===== Examples of WRONG predictions =====\n",
      "\n",
      "Text: All sponges were counted encountered for as were sutures.\n",
      "Gold: [[49, 55, 'DRUG']]\n",
      "Pred: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  Standard NER evaluation\n",
    "# =========================================\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from ner.utils import extract_spans, get_label_lists, remove_overlapping_spans\n",
    "\n",
    "# 1. Load JSON annotations\n",
    "test_annotations_path = PATH_TO_ANNOTATIONS + \"annotated_samples_test.json\"\n",
    "\n",
    "sentences = None # will become a list of sentences\n",
    "gold_spans = None # will become a nested list of 1 list per sentence, containing a list of [start, end, label]\n",
    "with open(test_annotations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\tannotations_dict = json.load(f)\n",
    "\tsentences = [sent for sent, _ in annotations_dict[\"annotations\"]]\n",
    "\tgold_spans = [entities_dict[\"entities\"] for _, entities_dict in annotations_dict[\"annotations\"]]\n",
    "\n",
    "# Remove overlapping spans in gold if any\n",
    "gold_spans = remove_overlapping_spans(gold_spans)\n",
    "print(f\"Loaded {len(sentences)} annotated sentences\")\n",
    "\n",
    "# 2. Load baseline spaCy NER\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# 3. Convert annotations to character-level spans\n",
    "pred_spans = extract_spans(nlp, sentences)\n",
    "\n",
    "# 4. Convert spans to entity sets for evaluation\n",
    "true_labels, pred_labels, associated_sentence_idx = get_label_lists(gold_spans, pred_spans)\n",
    "\n",
    "# 5. Evaluate macro and micro F1\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "\ttrue_labels, pred_labels, average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n===== Baseline spaCy NER Evaluation =====\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 score:  {f1:.4f}\")\n",
    "\n",
    "prec_m, rec_m, f1_m, _ = precision_recall_fscore_support(\n",
    "\ttrue_labels, pred_labels, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nMacro Precision:\", round(prec_m, 4))\n",
    "print(\"Macro Recall:   \", round(rec_m, 4))\n",
    "print(\"Macro F1:       \", round(f1_m, 4))\n",
    "\n",
    "# 6. Show some error cases\n",
    "print(\"\\n===== Examples of WRONG predictions =====\\n\")\n",
    "for sent_id, (g, p) in enumerate(zip(gold_spans, pred_spans)):\n",
    "    if g != p:\n",
    "        print(\"Text:\", sentences[sent_id])\n",
    "        print(\"Gold:\", g)\n",
    "        print(\"Pred:\", p)\n",
    "        print(\"-\" * 50)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26c084",
   "metadata": {},
   "source": [
    "## 4. Extend the standard NER types using the NER Annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7b2181",
   "metadata": {},
   "source": [
    "### 4.1 Training Extended NER model with >100 manual annoatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5d44df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 84 annotated sentences\n",
      "Custom NER labels: ['SYMPTOM', 'BODY_PART', 'DISEASE', 'DRUG', 'ROUTE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Esther\\anaconda3\\envs\\nlp-esther\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Then, CAT scan models were used to find tune and a...\" with entities \"[[89, 96, 'BODY_PART']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Esther\\anaconda3\\envs\\nlp-esther\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The tissues were expanded then with a tissue Metze...\" with entities \"[[205, 211, 'DRUG']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Esther\\anaconda3\\envs\\nlp-esther\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The sinus tarsi was then identified using a U-shap...\" with entities \"[[4, 9, 'BODY_PART'], [66, 72, 'BODY_PART']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Esther\\anaconda3\\envs\\nlp-esther\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The wound was closed in layers with PDS sutures.\" with entities \"[[40, 46, 'DRUG']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Esther\\anaconda3\\envs\\nlp-esther\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The periosteal flap was sutured over the staple us...\" with entities \"[[24, 30, 'DRUG'], [58, 64, 'DRUG']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Esther\\anaconda3\\envs\\nlp-esther\\lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Nasal trachea intubation was performed per routine...\" with entities \"[[8, 12, 'SYMPTOM']]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35 Loss: {'ner': np.float32(1041.8917)}\n",
      "Epoch 2/35 Loss: {'ner': np.float32(225.54295)}\n",
      "Epoch 3/35 Loss: {'ner': np.float32(137.1597)}\n",
      "Epoch 4/35 Loss: {'ner': np.float32(123.322716)}\n",
      "Epoch 5/35 Loss: {'ner': np.float32(109.89136)}\n",
      "Epoch 6/35 Loss: {'ner': np.float32(90.60602)}\n",
      "Epoch 7/35 Loss: {'ner': np.float32(65.435486)}\n",
      "Epoch 8/35 Loss: {'ner': np.float32(49.276722)}\n",
      "Epoch 9/35 Loss: {'ner': np.float32(41.300846)}\n",
      "Epoch 10/35 Loss: {'ner': np.float32(30.858707)}\n",
      "Epoch 11/35 Loss: {'ner': np.float32(34.177063)}\n",
      "Epoch 12/35 Loss: {'ner': np.float32(28.06081)}\n",
      "Epoch 13/35 Loss: {'ner': np.float32(20.424911)}\n",
      "Epoch 14/35 Loss: {'ner': np.float32(11.390415)}\n",
      "Epoch 15/35 Loss: {'ner': np.float32(9.220827)}\n",
      "Epoch 16/35 Loss: {'ner': np.float32(3.7878766)}\n",
      "Epoch 17/35 Loss: {'ner': np.float32(2.4475176)}\n",
      "Epoch 18/35 Loss: {'ner': np.float32(0.8665091)}\n",
      "Epoch 19/35 Loss: {'ner': np.float32(0.53081256)}\n",
      "Epoch 20/35 Loss: {'ner': np.float32(1.9285018)}\n",
      "Epoch 21/35 Loss: {'ner': np.float32(0.020747647)}\n",
      "Epoch 22/35 Loss: {'ner': np.float32(0.010263708)}\n",
      "Epoch 23/35 Loss: {'ner': np.float32(0.0034703491)}\n",
      "Epoch 24/35 Loss: {'ner': np.float32(0.8603833)}\n",
      "Epoch 25/35 Loss: {'ner': np.float32(0.01903076)}\n",
      "Epoch 26/35 Loss: {'ner': np.float32(0.38215208)}\n",
      "Epoch 27/35 Loss: {'ner': np.float32(3.4861627e-05)}\n",
      "Epoch 28/35 Loss: {'ner': np.float32(0.0024399434)}\n",
      "Epoch 29/35 Loss: {'ner': np.float32(0.0044349525)}\n",
      "Epoch 30/35 Loss: {'ner': np.float32(0.00020797655)}\n",
      "Epoch 31/35 Loss: {'ner': np.float32(0.12916152)}\n",
      "Epoch 32/35 Loss: {'ner': np.float32(0.40090063)}\n",
      "Epoch 33/35 Loss: {'ner': np.float32(2.0051759e-05)}\n",
      "Epoch 34/35 Loss: {'ner': np.float32(0.011705367)}\n",
      "Epoch 35/35 Loss: {'ner': np.float32(0.103435755)}\n",
      "Model saved to output_medical_ner\n",
      "\n",
      "Test sentence: Once the foot was reduced a Steinman pin was used to hold it in position.\n",
      "Predicted NER: [('foot', 'BODY_PART')]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  Extended NER TRAINING\n",
    "# =========================================\n",
    "\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# 1. Load annotated JSONL\n",
    "json_path = PATH_TO_ANNOTATIONS + \"annotated_samples_train.json\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\tannotations_dict = json.load(f)\n",
    "\ttraining_examples = annotations_dict[\"annotations\"]\n",
    "\t\n",
    "# Remove overlapping spans in training examples\n",
    "training_examples = remove_overlapping_spans(training_examples, is_annotations_list=True)\n",
    "print(f\"Loaded {len(training_examples)} annotated sentences\")\n",
    "\n",
    "# 2. Custom labels\n",
    "CUSTOM_LABELS = [\"SYMPTOM\", \"BODY_PART\", \"DISEASE\", \"DRUG\", \"ROUTE\"]\n",
    "assert annotations_dict['classes'] == CUSTOM_LABELS\n",
    "print(\"Custom NER labels:\", CUSTOM_LABELS)\n",
    "\n",
    "# 3. Initialize blank model for spaCy 3.8+\n",
    "nlp = spacy.blank(\"en\")         \n",
    "ner = nlp.add_pipe(\"ner\")       \n",
    "\n",
    "# Add custom labels\n",
    "for label in CUSTOM_LABELS:\n",
    "\tner.add_label(label)\n",
    "\n",
    "# 5. Training loop\n",
    "n_iter = 35\n",
    "optimizer = nlp.initialize()\n",
    "\n",
    "for epoch in range(n_iter):\n",
    "\trandom.shuffle(training_examples)\n",
    "\tlosses = {}\n",
    "\n",
    "\tbatches = minibatch(training_examples, size=compounding(4.0, 32.0, 1.5))\n",
    "\n",
    "\tfor batch in batches:\n",
    "\t\texamples = [Example.from_dict(nlp.make_doc(text), ann) for text, ann in batch]\n",
    "\t\tnlp.update(examples, sgd=optimizer, drop=0.2, losses=losses)\n",
    "\n",
    "\tprint(f\"Epoch {epoch+1}/{n_iter} Loss: {losses}\")\n",
    "\n",
    "# 6. Save model\n",
    "output_dir = \"output_medical_ner\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Model saved to\", output_dir)\n",
    "\n",
    "# 7. Quick sanity check\n",
    "test_text = random.choice(sentences)\n",
    "doc = nlp(test_text)\n",
    "\n",
    "print(\"\\nTest sentence:\", test_text)\n",
    "print(\"Predicted NER:\", [(ent.text, ent.label_) for ent in doc.ents])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f229beae",
   "metadata": {},
   "source": [
    "### 4.2 Extended NER Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36d2a3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 36 annotated sentences\n",
      "\n",
      "===== Extended spaCy NER Evaluation =====\n",
      "Precision: 0.6522\n",
      "Recall:    0.6522\n",
      "F1 score:  0.6522\n",
      "\n",
      "Macro Precision: 0.5466\n",
      "Macro Recall:    0.4343\n",
      "Macro F1:        0.4592\n",
      "\n",
      "===== Examples of WRONG predictions =====\n",
      "\n",
      "Text: Once the bone was harvested, surgical templets were used to recontour initially the maxillary graft and the mandibular graft.\n",
      "Gold: [[84, 91, 'BODY_PART'], [9, 13, 'BODY_PART']]\n",
      "Pred: [[9, 13, 'BODY_PART']]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  Extended NER evaluation with test sample\n",
    "# =========================================\n",
    "import json\n",
    "import spacy\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from ner.utils import extract_spans, get_label_lists, remove_overlapping_spans\n",
    "\n",
    "# 1. Load JSONL annotations for test\n",
    "test_annotations_path = PATH_TO_ANNOTATIONS + \"annotated_samples_test.json\"\n",
    "\n",
    "sentences = None # will become a list of sentences\n",
    "gold_spans = None # will become a nested list of 1 list per sentence, containing a list of [start, end, label]\n",
    "with open(test_annotations_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\tannotations_dict = json.load(f)\n",
    "\tsentences = [sent for sent, _ in annotations_dict[\"annotations\"]]\n",
    "\tgold_spans = [entities_dict[\"entities\"] for _, entities_dict in annotations_dict[\"annotations\"]]\n",
    "\n",
    "# Remove overlapping spans in gold if any\n",
    "gold_spans = remove_overlapping_spans(gold_spans)\n",
    "print(f\"Loaded {len(sentences)} annotated sentences\")\n",
    "\n",
    "# 2. Load Extended spaCy NER\n",
    "nlp = spacy.load(\"output_medical_ner\")\n",
    "\n",
    "# 3. Convert annotations to character-level spans\n",
    "pred_spans = extract_spans(nlp, sentences)\n",
    "\n",
    "# 4. Convert spans to entity sets for evaluation\n",
    "true_labels, pred_labels, associated_sentence_idx = get_label_lists(gold_spans, pred_spans)\n",
    "# 5. Evaluate macro and micro F1\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "\ttrue_labels, pred_labels, average=\"micro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n===== Extended spaCy NER Evaluation =====\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall:    {rec:.4f}\")\n",
    "print(f\"F1 score:  {f1:.4f}\")\n",
    "\n",
    "prec_m, rec_m, f1_m, _ = precision_recall_fscore_support(\n",
    "\ttrue_labels, pred_labels, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nMacro Precision:\", round(prec_m, 4))\n",
    "print(\"Macro Recall:   \", round(rec_m, 4))\n",
    "print(\"Macro F1:       \", round(f1_m, 4))\n",
    "\n",
    "# 6. Show some error cases\n",
    "print(\"\\n===== Examples of WRONG predictions =====\\n\")\n",
    "\n",
    "for sent_id, (g, p) in enumerate(zip(gold_spans, pred_spans)):\n",
    "    if g != p:\n",
    "        print(\"Text:\", sentences[sent_id])\n",
    "        print(\"Gold:\", g)\n",
    "        print(\"Pred:\", p)\n",
    "        print(\"-\" * 50)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed847e",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- The performance is poor as the train and test data sets are not in the same category, and therefore don't share big enough NER vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f3059d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Checking Exact Leakage =====\n",
      "EXACT LEAKAGE FOUND:\n",
      " - Maxillary atrophy.\n",
      " - ,2.\n",
      " - Severe mandibular atrophy.\n",
      " - Acquired facial deformity.\n",
      " - ,3.\n",
      "\n",
      "===== Checking Substring Leakage =====\n",
      "SUBSTRING LEAKAGE FOUND:\n",
      " - Maxillary atrophy.\n",
      " - ,2.\n",
      " - Severe mandibular atrophy.\n",
      " - Acquired facial deformity.\n",
      " - ,3.\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  DETECT TRAIN/TEST DATA LEAKAGE\n",
    "# =========================================\n",
    "\n",
    "# Load train sentences\n",
    "train_json = PATH_TO_ANNOTATIONS + \"annotated_samples_train.json\"\n",
    "with open(train_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "train_sentences = [text for text, _ in train_data[\"annotations\"]]\n",
    "\n",
    "# Load test sentences\n",
    "test_json = PATH_TO_ANNOTATIONS + \"annotated_samples_test.json\"\n",
    "with open(test_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "test_sentences = [text for text, _ in test_data[\"annotations\"]]\n",
    "\n",
    "# Exact-match leakage\n",
    "print(\"\\n===== Checking Exact Leakage =====\")\n",
    "leak_exact = []\n",
    "for t in test_sentences:\n",
    "    if t in train_sentences:\n",
    "        leak_exact.append(t)\n",
    "\n",
    "if leak_exact:\n",
    "    print(\"EXACT LEAKAGE FOUND:\")\n",
    "    for s in leak_exact:\n",
    "        print(\" -\", s)\n",
    "else:\n",
    "    print(\"No exact leakage.\")\n",
    "\n",
    "\n",
    "# Substring leakage (weaker but still harmful)\n",
    "print(\"\\n===== Checking Substring Leakage =====\")\n",
    "leak_sub = []\n",
    "for test_s in test_sentences:\n",
    "    for train_s in train_sentences:\n",
    "        if test_s.strip() in train_s.strip() or train_s.strip() in test_s.strip():\n",
    "            leak_sub.append(test_s)\n",
    "            break\n",
    "\n",
    "if leak_sub:\n",
    "    print(\"SUBSTRING LEAKAGE FOUND:\")\n",
    "    for s in leak_sub:\n",
    "        print(\" -\", s)\n",
    "else:\n",
    "    print(\"No substring leakage.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa558d",
   "metadata": {},
   "source": [
    "## 5. LLM-based NER classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6a5a4",
   "metadata": {},
   "source": [
    "The goal of this part was to see whether a large language model (LLM) could be used as an alternative NER system for my medical text, using only prompts and without training a new model. In practice, we tried several approaches, but all of them were limited by model size, hardware, or missing API access.\n",
    "\n",
    "First, we tried a very small local LLM (about 0.5B parameters) via `llama-cpp`. we asked it to extract entities with my label set (DISEASE, SYMPTOM, BODY_PART, FINDING, PROCEDURE) and return a JSON list. The model usually failed to follow the format and, more importantly, produced almost random labels that did not make medical sense. This suggests that such a small model is not strong enough for domain-specific NER, even with an instruction-style prompt.\n",
    "\n",
    "Second, we tried to use a slightly larger open-source LLM from Hugging Face (e.g. Qwen2.5-1.5B or Llama-3.2-1B) on my macOS machine via the `transformers` library. We implemented a function that builds a medical NER prompt and parses the model output into (phrase, label) pairs. However, even for one or two short sentences, generation on CPU/MPS took several minutes. Running this model on all test sentences to compute precision, recall and F1 would be impractically slow for this project.\n",
    "\n",
    "Third, we looked at the spaCy-LLM integration. In theory, we could define an `llm` component in a `config.cfg` file, for example using a Llama2 model on Hugging Face, and then assemble it as a normal spaCy pipeline. In practice, this either requires access to an external API (e.g. OpenAI) or downloading and running a much larger model (such as Llama2-7B). we have no API credits available and my local hardware is not sufficient to run such a model efficiently.\n",
    "\n",
    "Because of these limitations, we did not manage to build a fully working and scalable LLM-based NER baseline. Our “investigation” of LLM-based NER is therefore mainly conceptual: we explored how it would be prompted and integrated (with transformers or spaCy-LLM), and we observed that under my resource constraints it is not yet practical to use an LLM as a reliable, evaluated medical NER system. For the quantitative results in this project, we rely on the standard spaCy NER and my extended, manually trained medical NER model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2338a0fa",
   "metadata": {},
   "source": [
    "### 5.1 Example of NER with Llama-3.2-1B-Instruct on CPU\n",
    "\n",
    "**Skip this section if run on GPU**\n",
    "\n",
    "- It takes 20 min for two sentences. Therefore it is just a demo of how NER can be done. \n",
    "- If we have more computational resources, we will evaluate on the whole annotation test data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b97d4258",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from llama_cpp import Llama\n",
    "import json, re\n",
    "\n",
    "# 1. Load GGUF model (same as you already used)\n",
    "llm = Llama(\n",
    "\tmodel_path=\"models/Llama-3.2-1B-Instruct-f16.gguf\",\n",
    "\tn_ctx=2048,\n",
    "\tn_threads=6,\n",
    "\tn_gpu_layers=0     # works on Mac / Windows / Linux\n",
    ")\n",
    "\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c6a9b20",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 2. Test sentence\n",
    "text = \"She was having chest pains along with significant vomiting and diarrhea.\"\n",
    "\n",
    "# 3. Minimal zero-shot NER prompt (no enhancements)\n",
    "PROMPT = \"\"\"\n",
    "Extract medical named entities from the text.\n",
    "Use only these labels: DISEASE, SYMPTOM, BODY_PART, FINDING, PROCEDURE.\n",
    "Return ONLY a JSON list like this:\n",
    "\"entity_text_1\":  \"LABEL_1\",\n",
    "Text:\n",
    "{TEXT}\n",
    "\"\"\"\n",
    "\n",
    "# 4. Run LLM\n",
    "raw = llm(PROMPT.format(TEXT=text), max_tokens=256)\n",
    "output = raw[\"choices\"][0][\"text\"]\n",
    "print(\"\\nRaw LLM output:\\n\", output)\n",
    "\n",
    "# 5. Parse JSON from model output\n",
    "def parse_json(s):\n",
    "\ttry:\n",
    "\t\tmatch = re.search(r\"\\[.*\\]\", s, re.S)\n",
    "\t\tif match:\n",
    "\t\t\treturn json.loads(match.group(0))\n",
    "\texcept:\n",
    "\t\tpass\n",
    "\treturn []\n",
    "\n",
    "ents = parse_json(output)\n",
    "\n",
    "# 6. Print final extracted entities\n",
    "print(\"\\nParsed entities:\")\n",
    "for e in ents:\n",
    "\tprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a26c1e",
   "metadata": {},
   "source": [
    "## 5.2 Sanity Check of LLM Behavior with a Minimal n-Shot Prompt\n",
    "\n",
    "Before relying on spaCy-LLM’s built-in NER prompting strategy, it is important to understand how the base LLM behaves under a transparent and fully interpretable prompt. This sanity check serves several purposes:\n",
    "\n",
    "- LLMs are prone to hallucination, especially when forced to choose from a restricted label set. Observing their raw behavior helps identify systematic failure modes such as over-labeling or speculative predictions.\n",
    "- The default spaCy-LLM prompts are highly engineered and not fully documented. Since we cannot see their internal design decisions, it is useful to test a simple and fully understandable prompt for comparison.\n",
    "- Our custom few-shot examples do not (and should not) follow spaCy-LLM’s templating rules. The purpose here is not to replace spaCy-LLM, but to benchmark the raw LLM’s baseline behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950a8cb",
   "metadata": {},
   "source": [
    "### 5.2.1 Log of simple-prompt llm-based NER"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2047ef38",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ============== CONFIG ==============\n",
    "\n",
    "TEST_JSON_PATH = \"ner/samples/annotated_samples_test_1.json\"\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "LOG_PATH = \"ner/sanity_check_llm/mistral_ner_log.txt\"  # new: log file\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Few-shot examples\n",
    "# ----- Few-shot examples (use your current best EXAMPLES) -----\n",
    "EXAMPLES = \"\"\"\n",
    "\n",
    "Input Text: \",The estimated blood loss in the harvest of the hip was 100 cc.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "None\n",
    "\n",
    "Input Text: \"He has been having significant feet pain with significant planovalgus deformity.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "- \"pain\" | SYMPTOM\n",
    "- \"deformity\" | SYMPTOM\n",
    "- \"planovalgus\" | DISEASE\n",
    "\n",
    "Input Text: \"A surgical mallet then compressed this bone further into the region.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "- \"bone\" | BODY_PART\n",
    "\n",
    "Input Text: \"A primary incision was made between the mental foramina and the residual crest of the ridge and reflected first to the lingual area observing the superior genial tubercle in the facial area degloving the mentalis muscle and exposing the anterior body.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "- \"mental foramina\" | BODY_PART\n",
    "- \"genial tubercle\" | BODY_PART\n",
    "\n",
    "Input Text: \"The patient was noted to have flexible vertical talus.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "- \"vertical talus\" | DISEASE\n",
    "\n",
    "Input Text: \"A piece of AlloDerm mixed with Croften and patient's platelet-rich plasma, which was centrifuged from drawing 20 cc of blood was then mixed together and placed over the lateral aspect of the block.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "- \"AlloDerm\" | DRUG\n",
    "\n",
    "Input Text: \"The area was injected with 6 mL of 0.25% Marcaine local anesthetic.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "- \"Marcaine\" | DRUG\n",
    "\n",
    "Input Text: \"The estimated blood loss in the intraoral procedure was 220 cc.\"\n",
    "Labels: SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE\n",
    "Output:\n",
    "- \"intraoral\" | ROUTE\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_TEMPLATE = f\"\"\"\n",
    "Here is a Text:\n",
    "{{TEXT}}\n",
    "\n",
    "Check whether or not there are medical words in the Text that can be labelled\n",
    "and ONLY labeled as one of the labels SYMPTOM, BODY_PART, DISEASE, DRUG, ROUTE in a medical sense.\n",
    "If not, confidently report None.\n",
    "If so, report following the format below: \n",
    "- \"actual word in the input Text\" | LABEL\n",
    "\n",
    "Use the strict definitions for the labels as follows.\n",
    "SYMPTOM: Phrases that describe what the patient feels or observable clinical signs \n",
    "         (e.g. pain, swelling, dysfunction, shortness of breath).\n",
    "\n",
    "\n",
    "BODY_PART: Names of anatomical body structures or regions \n",
    "         (e.g. bone, mandible, forearm, Achilles tendon).\n",
    "\n",
    "DISEASE: Official disease or diagnosis terms, usually multi-word or technical names \n",
    "         (e.g. congenital myotonic muscular dystrophy, planovalgus).\n",
    "\n",
    "DRUG: Names of medications, anesthetics, or pharmacological agents \n",
    "      (e.g. Marcaine, Xylocaine, morphine).\n",
    "\n",
    "ROUTE: Words that describe the route of administration into the body \n",
    "       (e.g. IV, intraoral, intramuscular, intravenously).\n",
    "\n",
    "\n",
    "NEVER mention any word that is not in the raw Text.\n",
    "\n",
    "Learn from some examples below.\n",
    "\n",
    "{EXAMPLES}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\" if device == \"cuda\" else None,\n",
    ")\n",
    "\n",
    "\n",
    "# ============== Helper: LLM NER prediction ==============\n",
    "\n",
    "def llm_ner_predict(text, max_new_tokens=256):\n",
    "    prompt = PROMPT_TEMPLATE.format(TEXT=text)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    answer = output_text[len(prompt):].strip()\n",
    "\n",
    "    pattern = r'-\\s*\"([^\"]+)\"\\s*\\|\\s*([A-Z_]+)'\n",
    "    matches = re.findall(pattern, answer)\n",
    "    preds = [(t, lab) for t, lab in matches]\n",
    "\n",
    "    return preds, answer\n",
    "\n",
    "\n",
    "# ============== Load test dataset ==============\n",
    "\n",
    "with open(TEST_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "annotations = data[\"annotations\"]\n",
    "test_count = len(annotations)\n",
    "\n",
    "print(f\"Loaded {test_count} test examples.\")\n",
    "\n",
    "os.makedirs(os.path.dirname(LOG_PATH), exist_ok=True)\n",
    "\n",
    "# ============== Write log header ==============\n",
    "\n",
    "with open(LOG_PATH, \"w\", encoding=\"utf-8\") as log_f:\n",
    "    log_f.write(f\"Loaded {test_count} test examples.\\n\\n\")\n",
    "\n",
    "    log_f.write(\"===== MODEL & DATA INFO =====\\n\")\n",
    "    log_f.write(f\"MODEL_ID: {MODEL_ID}\\n\")\n",
    "    log_f.write(f\"TEST_JSON_PATH: {TEST_JSON_PATH}\\n\\n\")\n",
    "\n",
    "    log_f.write(\"===== PROMPT TEMPLATE =====\\n\")\n",
    "    log_f.write(PROMPT_TEMPLATE)\n",
    "    log_f.write(\"\\n\\n===== START EXAMPLES =====\\n\\n\")\n",
    "\n",
    "    # ============== Iterate over test samples ==============\n",
    "\n",
    "    for idx, (sent, ann_dict) in enumerate(annotations):\n",
    "        gold_spans = ann_dict.get(\"entities\", [])\n",
    "\n",
    "        gold_entities = []\n",
    "        for start, end, label in gold_spans:\n",
    "            gold_entities.append((sent[start:end], label))\n",
    "\n",
    "        preds, raw_output = llm_ner_predict(sent)\n",
    "\n",
    "        block = []\n",
    "        block.append(\"=\" * 80 + \"\\n\")\n",
    "        block.append(f\"Example {idx+1}/{test_count}\\n\")\n",
    "        block.append(f\"TEXT:\\n{sent}\\n\\n\")\n",
    "\n",
    "        block.append(\"GOLD ENTITIES (string level):\\n\")\n",
    "        if gold_entities:\n",
    "            for text_span, lab in gold_entities:\n",
    "                block.append(f\"- '{text_span}' | {lab}\\n\")\n",
    "        else:\n",
    "            block.append(\"- None\\n\")\n",
    "\n",
    "        block.append(\"\\nLLM PREDICTIONS (parsed):\\n\")\n",
    "        if preds:\n",
    "            for t, lab in preds:\n",
    "                block.append(f\"- '{t}' | {lab}\\n\")\n",
    "        else:\n",
    "            block.append(\"- None\\n\")\n",
    "\n",
    "        block.append(\"\\nRAW LLM OUTPUT (for debugging):\\n\")\n",
    "        block.append(raw_output + \"\\n\")\n",
    "        block.append(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        block_str = \"\".join(block)\n",
    "\n",
    "        # Print to console\n",
    "        print(block_str, end=\"\")\n",
    "\n",
    "        # Write to log\n",
    "        log_f.write(block_str)\n",
    "\n",
    "print(f\"\\nLog written to: {LOG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b27c0",
   "metadata": {},
   "source": [
    "### 5.2.2 Evaluation of simple-prompt llm-based NER"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50710fb5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# =========================================\n",
    "#  LLM NER Evaluation: Precision/Recall/F1\n",
    "# =========================================\n",
    "\n",
    "import re\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def find_span_in_text(text, substring):\n",
    "    \"\"\"\n",
    "    Return (start, end) of the first exact match of substring in text.\n",
    "    If not found, return None.\n",
    "    Case-sensitive.\n",
    "    \"\"\"\n",
    "    pattern = re.escape(substring)\n",
    "    match = re.search(pattern, text)\n",
    "    if not match:\n",
    "        return None\n",
    "    return match.start(), match.end()\n",
    "\n",
    "\n",
    "# Convert gold spans into evaluation form\n",
    "gold_eval = []  # list of lists: [ [(start,end,label), ...], ... ]\n",
    "for sent, ann in annotations:\n",
    "    spans = []\n",
    "    for start, end, label in ann[\"entities\"]:\n",
    "        spans.append((start, end, label))\n",
    "    gold_eval.append(spans)\n",
    "\n",
    "\n",
    "# Convert LLM preds (string spans) into character spans\n",
    "pred_eval = []  # same structure as gold_eval\n",
    "for idx, (sent, ann) in enumerate(annotations):\n",
    "    preds, _ = llm_ner_predict(sent)\n",
    "    sent_pred_spans = []\n",
    "\n",
    "    for ent_text, label in preds:\n",
    "        span = find_span_in_text(sent, ent_text)\n",
    "        if span is None:\n",
    "            continue  # skip unmatched spans\n",
    "        start, end = span\n",
    "        sent_pred_spans.append((start, end, label))\n",
    "\n",
    "    pred_eval.append(sent_pred_spans)\n",
    "\n",
    "\n",
    "# ========== Construct label lists for PRF computation ==========\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for gold_spans, pred_spans in zip(gold_eval, pred_eval):\n",
    "\n",
    "    gold_set = {(s, e, l) for (s, e, l) in gold_spans}\n",
    "    pred_set = {(s, e, l) for (s, e, l) in pred_spans}\n",
    "\n",
    "    # True positives\n",
    "    for span in gold_set.intersection(pred_set):\n",
    "        true_labels.append(span[2])\n",
    "        pred_labels.append(span[2])\n",
    "\n",
    "    # False negatives\n",
    "    for span in gold_set - pred_set:\n",
    "        true_labels.append(span[2])\n",
    "        pred_labels.append(\"NONE\")\n",
    "\n",
    "    # False positives\n",
    "    for span in pred_set - gold_set:\n",
    "        true_labels.append(\"NONE\")\n",
    "        pred_labels.append(span[2])\n",
    " \n",
    "  \n",
    "# ========== Compute micro/macro F1 ==========\n",
    "\n",
    "prec_micro, rec_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"micro\", zero_division=0\n",
    ")\n",
    "prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "    true_labels, pred_labels, average=\"macro\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n===== LLM NER Evaluation =====\")\n",
    "print(f\"Micro Precision: {prec_micro:.4f}\")\n",
    "print(f\"Micro Recall:    {rec_micro:.4f}\")\n",
    "print(f\"Micro F1 score:  {f1_micro:.4f}\")\n",
    "\n",
    "print(\"\\nMacro Precision:\", round(prec_macro, 4))\n",
    "print(\"Macro Recall:   \", round(rec_macro, 4))\n",
    "print(\"Macro F1:       \", round(f1_macro, 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28f2434",
   "metadata": {},
   "source": [
    "### Summary of minimal n-shot prompt results\n",
    "\n",
    "Despite including clear and relevant examples, the raw LLM under a simple n-shot prompt achieved only:\n",
    "\n",
    "Micro F1 score:  0.3043\n",
    "Macro F1:        0.2228\n",
    "\n",
    "This confirms that unconstrained prompting is not sufficient for reliable NER, and that hallucination control and output normalization require more structure than a minimal prompt can provide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f712059a",
   "metadata": {},
   "source": [
    "## 5.3 Evaluation of the spaCy-LLM Pipeline\n",
    "\n",
    "The spaCy-LLM framework adds an essential layer of structure around the base LLM. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a0ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names avaible: dolly-v2-3b Llama-2-13b-hf mistral-7b (more to be explored) original\n",
    "!python -m ner.spacy_llm.load_model --model  mistral-7b\n",
    "!python -m ner.spacy_llm.evaluate --model mistral-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817a8c7",
   "metadata": {},
   "source": [
    "Current Annotation\n",
    "\n",
    "==== Micro-Averaged Metrics ====\n",
    "\n",
    "Precision: 0.1972\n",
    "\n",
    "Recall:    0.1972\n",
    "\n",
    "F1-score:  0.1972\n",
    "\n",
    "Macro Precision: 0.2220\n",
    "\n",
    "Macro Recall:   0.1636\n",
    "\n",
    "Macro F1:       0.1710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0bd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/homefs/kw24z021/miniconda3/envs/nlp-task2/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/storage/homefs/kw24z021/miniconda3/envs/nlp-task2/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|█████████████████| 2/2 [00:00<00:00, 105.35it/s]\n",
      "Model saved to /storage/homefs/kw24z021/NLP_LLM/group_project/MedNLP-Multitask/ner/spacy_llm/models/output_mistral-7b_ner\n",
      "Loaded 36 annotated test sentences.\n",
      "Loading model...mistral-7b\n",
      "/storage/homefs/kw24z021/miniconda3/envs/nlp-task2/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/storage/homefs/kw24z021/miniconda3/envs/nlp-task2/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|█████████████████| 2/2 [00:00<00:00, 104.55it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Prediction completed in 16.40 seconds for 36 sentences.\n",
      "\n",
      "==== Micro-Averaged Metrics ====\n",
      "Precision: 0.2429\n",
      "Recall:    0.2429\n",
      "F1-score:  0.2429\n",
      "\n",
      "Macro Precision: 0.4889\n",
      "Macro Recall:   0.3388\n",
      "Macro F1:       0.3388\n",
      "\n",
      "===== Examples of WRONG predictions =====\n",
      "\n",
      "Text: A posterior tunnel was done first on the left side along the mylohyoid ridge and then under retromolar pad to the external oblique and the ridge was then degloved.\n",
      "Gold: [[61, 76, 'BODY_PART'], [92, 106, 'BODY_PART'], [114, 130, 'BODY_PART']]\n",
      "Pred: [[41, 50, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Surgical team scrubbed and gowned in usual fashion and the patient was draped.\n",
      "Gold: []\n",
      "Pred: [[0, 13, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Maxillary atrophy.\n",
      "Gold: [[0, 9, 'BODY_PART'], [10, 17, 'SYMPTOM']]\n",
      "Pred: [[0, 9, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Once the bone was harvested, surgical templets were used to recontour initially the maxillary graft and the mandibular graft.\n",
      "Gold: [[9, 13, 'BODY_PART'], [84, 92, 'BODY_PART'], [108, 117, 'BODY_PART']]\n",
      "Pred: [[29, 46, 'DRUG']]\n",
      "--------------------------------------------------\n",
      "Text: The ankle was taken through a range of motion with noted improvement in the reduction of the talocalcaneal alignment with the foot in plantar flexion on the lateral view.\n",
      "Gold: [[4, 9, 'BODY_PART'], [126, 130, 'BODY_PART']]\n",
      "Pred: [[4, 9, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Intended incision was marked on the skin.\n",
      "Gold: []\n",
      "Pred: [[36, 40, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Risks of surgery include risks of anesthesia, infection, bleeding, changes in sensation and motion of the extremity, hardware failure, need for other surgical procedures, need to be nonweightbearing for some time.\n",
      "Gold: [[46, 55, 'SYMPTOM'], [57, 65, 'SYMPTOM'], [105, 115, 'BODY_PART']]\n",
      "Pred: [[34, 44, 'SYMPTOM']]\n",
      "--------------------------------------------------\n",
      "Text: This was explained to the mother in detail.\n",
      "Gold: []\n",
      "Pred: [[26, 32, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: The periosteal flap was sutured over the staple using 2-0 Vicryl.\n",
      "Gold: [[4, 19, 'BODY_PART'], [58, 64, 'DRUG']]\n",
      "Pred: [[4, 19, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: The tissues were stretched with tissue scissors and then a high speed instrumentation was used to decorticate the anterior mandible using a 1.6 mm twist drill and a pear shaped bur was used in the posterior region to begin original exploratory phenomenon of repair.\n",
      "Gold: [[123, 131, 'BODY_PART']]\n",
      "Pred: [[4, 11, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Once the foot was reduced a Steinman pin was used to hold it in position.\n",
      "Gold: [[9, 13, 'BODY_PART']]\n",
      "Pred: [[28, 40, 'DRUG']]\n",
      "--------------------------------------------------\n",
      "Text: Incision was then made over the left lateral aspect of the hind foot to expose the talocalcaneal joint.\n",
      "Gold: [[59, 68, 'BODY_PART'], [83, 102, 'BODY_PART']]\n",
      "Pred: [[59, 68, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: PREOPERATIVE DIAGNOSIS: , Congenital myotonic muscular dystrophy with bilateral planovalgus feet.,POSTOPERATIVE\n",
      "Gold: [[37, 64, 'DISEASE'], [80, 91, 'DISEASE'], [92, 96, 'BODY_PART']]\n",
      "Pred: []\n",
      "--------------------------------------------------\n",
      "Text: The cephalic vein was divided, and the proximal end was anastomosed to the artery in an end-to-side fashion with a running 6-0 Prolene suture.\n",
      "Gold: [[4, 17, 'BODY_PART'], [75, 81, 'BODY_PART'], [127, 134, 'DRUG']]\n",
      "Pred: [[4, 17, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Bilateral long-leg casts were then placed with the foot in neutral with some moulding of his medial plantar arch.\n",
      "Gold: [[51, 55, 'BODY_PART'], [93, 112, 'BODY_PART']]\n",
      "Pred: [[10, 24, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Severe mandibular atrophy.\n",
      "Gold: [[0, 7, 'SYMPTOM'], [8, 17, 'BODY_PART'], [18, 25, 'SYMPTOM']]\n",
      "Pred: [[7, 17, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Hemostasis was obtained.\n",
      "Gold: []\n",
      "Pred: [[0, 10, 'SYMPTOM']]\n",
      "--------------------------------------------------\n",
      "Text: Acquired facial deformity.\n",
      "Gold: [[0, 8, 'SYMPTOM'], [9, 15, 'BODY_PART'], [16, 25, 'SYMPTOM']]\n",
      "Pred: [[9, 15, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: The block of bone was further re-contoured in situ.\n",
      "Gold: [[13, 17, 'BODY_PART']]\n",
      "Pred: [[4, 17, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: TIME:  ,Tourniquet time was 53 minutes on the left and 45 minutes on the right.\n",
      "Gold: []\n",
      "Pred: [[8, 23, 'ROUTE']]\n",
      "--------------------------------------------------\n",
      "Text: A tunnel was formed in the posterior region separating the mental nerve artery and vein from the flap and exposing that aspect of the body of the mandible.\n",
      "Gold: [[59, 71, 'BODY_PART'], [72, 78, 'BODY_PART'], [83, 87, 'BODY_PART'], [146, 154, 'BODY_PART']]\n",
      "Pred: [[27, 36, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: The facial tissues were then reflected exposing the lateral aspect of the maxilla, the zygomatic arch, the infraorbital nerve, artery and vein, the lateral piriform rim, the inferior piriform rim, and the remaining issue of the nasal spine.\n",
      "Gold: [[74, 81, 'BODY_PART'], [87, 101, 'BODY_PART'], [107, 125, 'BODY_PART'], [148, 168, 'BODY_PART'], [174, 195, 'BODY_PART'], [228, 239, 'BODY_PART']]\n",
      "Pred: [[74, 81, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: Xylocaine 1%, 1:100,000 epinephrine 7 ml was infiltrated into the labial and palatal mucosa.\n",
      "Gold: [[0, 9, 'DRUG'], [24, 35, 'DRUG']]\n",
      "Pred: [[0, 9, 'DRUG']]\n",
      "--------------------------------------------------\n",
      "Text: The IV catheter was inserted into the vein on the lower surface of the left forearm.\n",
      "Gold: [[4, 6, 'ROUTE'], [38, 42, 'BODY_PART'], [76, 83, 'BODY_PART']]\n",
      "Pred: [[4, 15, 'ROUTE']]\n",
      "--------------------------------------------------\n",
      "Text: DIAGNOSIS: , Congenital myotonic muscular dystrophy with bilateral planovalgus feet.\n",
      "Gold: [[24, 51, 'DISEASE'], [67, 78, 'DISEASE'], [79, 83, 'BODY_PART']]\n",
      "Pred: []\n",
      "--------------------------------------------------\n",
      "\n",
      "===== Examples of CORRECT predictions =====\n",
      "\n",
      "Text: This was also checked with fluoroscopy.\n",
      "Gold: []\n",
      "Pred: []\n",
      "--------------------------------------------------\n",
      "Text: Masticatory dysfunction.,POSTOPERATIVE DIAGNOSES:,1.\n",
      "Gold: [[0, 23, 'SYMPTOM']]\n",
      "Pred: [[0, 23, 'SYMPTOM']]\n",
      "--------------------------------------------------\n",
      "Text: ,The clamps were removed establishing flow through the fistula.\n",
      "Gold: [[55, 62, 'DISEASE']]\n",
      "Pred: [[55, 62, 'DISEASE']]\n",
      "--------------------------------------------------\n",
      "Text: The fluid administered 300 cc.\n",
      "Gold: []\n",
      "Pred: []\n",
      "--------------------------------------------------\n",
      "Text: ,2.\n",
      "Gold: []\n",
      "Pred: []\n",
      "--------------------------------------------------\n",
      "Text: The incision was then extended posteriorly to allow for visualization of the Achilles, which was Z-lengthened with the release of the lateral distal half.\n",
      "Gold: [[77, 85, 'BODY_PART']]\n",
      "Pred: [[77, 85, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: #3-0 Gore-Tex.  Attention was brought then to the mandible.\n",
      "Gold: [[50, 58, 'BODY_PART']]\n",
      "Pred: [[50, 58, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: The patient received 6 mL of 0.25% Marcaine local anesthetic on each side.,TOURNIQUET\n",
      "Gold: [[35, 43, 'DRUG']]\n",
      "Pred: [[35, 43, 'DRUG']]\n",
      "--------------------------------------------------\n",
      "Text: ,3.\n",
      "Gold: []\n",
      "Pred: []\n",
      "--------------------------------------------------\n",
      "Text: Both the extremities were then prepped and draped in standard surgical fashion.\n",
      "Gold: [[9, 20, 'BODY_PART']]\n",
      "Pred: [[9, 20, 'BODY_PART']]\n",
      "--------------------------------------------------\n",
      "Text: The area was re-contoured with rongeurs.\n",
      "Gold: []\n",
      "Pred: []\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Try different example settings, combined with revised gold annotations\n",
    "\n",
    "# See example settings in ner_examples_cot_balanced_1.json, revised gold annotation in annotated_samples_test_1.json\n",
    "# Modify the .cfg and evaluation.py accordingly. \n",
    "#  \n",
    "# Model names avaible: dolly-v2-3b Llama-2-13b-hf mistral-7b (more to be explored)\n",
    "!python -m ner.spacy_llm.load_model --model  mistral-7b\n",
    "!python -m ner.spacy_llm.evaluate --model mistral-7b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bbddc",
   "metadata": {},
   "source": [
    "Revised gold annotation and examples\n",
    "\n",
    "==== Micro-Averaged Metrics ====\n",
    "\n",
    "Precision: 0.2429\n",
    "\n",
    "Recall:    0.2429\n",
    "\n",
    "F1-score:  0.2429\n",
    "\n",
    "Macro Precision: 0.4889\n",
    "\n",
    "Macro Recall:   0.3388\n",
    "\n",
    "Macro F1:       0.3388"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e1b041",
   "metadata": {},
   "source": [
    "\n",
    "### Summary\n",
    "- The comparison between minimal raw prompting and spaCy-LLM confirms the importance of architectural constraints and structured prompt design for LLM-based NER.\n",
    "\n",
    "When evaluated on the same test dataset, spaCy-LLM achieved:\n",
    "\n",
    "Micro F1 = 0.2063  \n",
    "Macro F1 = 0.2041\n",
    "\n",
    "Although performance is still modest, these results are significantly better than the minimal n-shot prompt baseline. This demonstrates that structured prompting, template enforcement, and standardized output parsing reduce hallucinations and improve entity extraction reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2367f6d8",
   "metadata": {},
   "source": [
    "## 6. How NER type information can help other NLP tasks\n",
    "\n",
    "Even if the NER models in this project are not perfect, the extracted entity types are still useful for many other NLP applications in the clinical domain. Here I briefly summarise a few examples.\n",
    "\n",
    "1. **Structuring free-text clinical notes**  \n",
    "\tClinical documents are mostly free text. NER can identify key concepts and turn them into structured fields, for example:\n",
    "\t- DISEASE: diabetes mellitus, hypertension, stasis ulcer  \n",
    "\t- SYMPTOM: chest pain, nausea, diarrhea  \n",
    "\t- BODY_PART: right ankle, abdomen  \n",
    "\t- PROCEDURE: surgery, CT scan  \n",
    "\tThis structured information can then be stored in an electronic health record or database and used for search, filtering, and statistics.\n",
    "\n",
    "2. **Clinical decision support**  \n",
    "\tNER outputs can be used as features in decision support systems. For example:\n",
    "\t- Combinations of DISEASE and SYMPTOM entities can be used to estimate the risk of certain conditions.\n",
    "\t- DRUG and DOSAGE entities can be checked for possible drug interactions or dosing errors.\n",
    "\tIn this way, NER acts as a bridge between narrative notes and automated clinical rules or prediction models.\n",
    "\n",
    "3. **Document and patient-level classification**  \n",
    "\tNER types can also improve text classification. Instead of using only bag-of-words, we can use counts and patterns of entities:\n",
    "\t- Classifying documents by specialty (e.g. cardiology vs. endocrinology) based on the diseases and body parts mentioned.\n",
    "\t- Detecting potential adverse drug events by looking for co-occurrences of specific DRUG and SYMPTOM entities.\n",
    "\tAt the patient level, the presence or absence of certain DISEASE entities can be used to define cohorts for research.\n",
    "\n",
    "4. **Relation extraction and knowledge graphs**  \n",
    "\tNER is the first step towards relation extraction, such as:\n",
    "\t- SYMPTOM–DISEASE relations (e.g. chest pain → myocardial infarction ruled out)\n",
    "\t- DRUG–DISEASE relations (indications)  \n",
    "\t- DRUG–SYMPTOM relations (adverse effects)  \n",
    "\tOnce entities are identified, these relations can be learned or manually defined, and combined into a clinical knowledge graph.\n",
    "\n",
    "5. **Question answering and summarisation**  \n",
    "\tFor question answering, knowing which spans are DISEASE or SYMPTOM helps focus retrieval on the relevant parts of a document. For summarisation, NER can be used to ensure that all important entities (diagnoses, symptoms, procedures, medications) appear explicitly in the final summary, even if the original note is long and repetitive.\n",
    "\n",
    "Overall, NER type information turns unstructured clinical text into more interpretable and reusable signals. Even a relatively noisy NER system can already provide useful features for downstream tasks such as decision support, classification, relation extraction, and summarisation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d384ec1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbbef8af",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
